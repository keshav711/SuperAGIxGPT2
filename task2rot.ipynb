{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff381f9",
   "metadata": {},
   "source": [
    "**Task 2** | \n",
    "1.Rotary embeddings - so in RoFormer  the limitation of direction is reduced by introducing a rotated attention mechanism. \n",
    "In this mechanism, the attention is allowed to focus not only on positions to the left and right but also on positions above and below the current position. \n",
    "conclusion -- This modification enables the model to capture diagonal relationships in the input sequence, which can be particularly useful for tasks where such dependencies are crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbc5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#for testing of my implementation\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "from math import pi, log\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import nn, einsum, broadcast_tensors\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "import torch.utils.checkpoint\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e840b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "     -------------------------------------- 44.6/44.6 kB 731.0 kB/s eta 0:00:00\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fa3e4",
   "metadata": {},
   "source": [
    "Errors faced in TASK 2:\n",
    "\n",
    "Can't test the implementation due to time constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8ea8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesha\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "# learned rotation helpers\n",
    "\n",
    "def apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n",
    "    if exists(freq_ranges):\n",
    "        rotations = einsum('..., f -> ... f', rotations, freq_ranges)\n",
    "        rotations = rearrange(rotations, '... r f -> ... (r f)')\n",
    "\n",
    "    rotations = repeat(rotations, '... n -> ... (n r)', r = 2)\n",
    "    return apply_rotary_emb(rotations, t, start_index = start_index)\n",
    "\n",
    "@autocast(enabled = False)\n",
    "def apply_rotary_emb(freqs, t, start_index = 0, scale = 1., seq_dim = -2):\n",
    "    rot_dim, seq_len = freqs.shape[-1], t.shape[seq_dim]\n",
    "    freqs = freqs[-seq_len:].to(t)\n",
    "\n",
    "    end_index = start_index + rot_dim\n",
    "    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'\n",
    "    t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n",
    "    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n",
    "    return torch.cat((t_left, t, t_right), dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2d4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def broadcat(tensors, dim = -1):\n",
    "    broadcasted_tensors = broadcast_tensors(*tensors)\n",
    "    return torch.cat(broadcasted_tensors, dim = dim)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n",
    "    x1, x2 = x.unbind(dim = -1)\n",
    "    x = torch.stack((-x2, x1), dim = -1)\n",
    "    return rearrange(x, '... d r -> ... (d r)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c6eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        custom_freqs = None,\n",
    "        freqs_for = 'lang',\n",
    "        theta = 10000,\n",
    "        max_freq = 10,\n",
    "        num_freqs = 1,\n",
    "        learned_freq = False,\n",
    "        use_xpos = False,\n",
    "        xpos_scale_base = 512,\n",
    "        interpolate_factor = 1.,\n",
    "        theta_rescale_factor = 1.,\n",
    "        seq_before_head_dim = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        theta *= theta_rescale_factor ** (dim / (dim - 2))\n",
    "\n",
    "        if exists(custom_freqs):\n",
    "            freqs = custom_freqs\n",
    "        elif freqs_for == 'lang':\n",
    "            freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "        elif freqs_for == 'pixel':\n",
    "            freqs = torch.linspace(1., max_freq / 2, dim // 2) * pi\n",
    "        elif freqs_for == 'constant':\n",
    "            freqs = torch.ones(num_freqs).float()\n",
    "        else:\n",
    "            raise ValueError(f'unknown modality {freqs_for}')\n",
    "\n",
    "        self.cache = dict()\n",
    "        self.cache_scale = dict()\n",
    "        self.freqs = nn.Parameter(freqs, requires_grad = learned_freq)\n",
    "\n",
    "        self.learned_freq = learned_freq\n",
    "\n",
    "        # default sequence dimension\n",
    "\n",
    "        self.seq_before_head_dim = seq_before_head_dim\n",
    "        self.default_seq_dim = -3 if seq_before_head_dim else -2\n",
    "\n",
    "        # interpolation factors\n",
    "\n",
    "        assert interpolate_factor >= 1.\n",
    "        self.interpolate_factor = interpolate_factor\n",
    "\n",
    "        # xpos\n",
    "\n",
    "        self.use_xpos = use_xpos\n",
    "        if not use_xpos:\n",
    "            self.register_buffer('scale', None)\n",
    "            return\n",
    "\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.scale_base = xpos_scale_base\n",
    "        self.register_buffer('scale', scale)\n",
    "\n",
    "    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n",
    "        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n",
    "\n",
    "    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, freq_seq_len = None):\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        assert not self.use_xpos, 'you must use `.rotate_queries_and_keys` method instead and pass in both queries and keys, for length extrapolatable rotary embeddings'\n",
    "\n",
    "        device, dtype, seq_len = t.device, t.dtype, t.shape[seq_dim]\n",
    "\n",
    "        if exists(freq_seq_len):\n",
    "            assert freq_seq_len >= seq_len\n",
    "            seq_len = freq_seq_len\n",
    "\n",
    "        freqs = self.forward(lambda: self.get_seq_pos(seq_len, device = device, dtype = dtype, offset = offset), cache_key = f'freqs:{seq_len}|offset:{offset}')\n",
    "\n",
    "        if seq_dim == -3:\n",
    "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
    "\n",
    "        return apply_rotary_emb(freqs, t, seq_dim = seq_dim)\n",
    "\n",
    "    def rotate_queries_with_cached_keys(self, q, k, seq_dim = None, offset = 0):\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        q_len, k_len = q.shape[seq_dim], k.shape[seq_dim]\n",
    "        assert q_len <= k_len\n",
    "        rotated_q = self.rotate_queries_or_keys(q, seq_dim = seq_dim, freq_seq_len = k_len)\n",
    "        rotated_k = self.rotate_queries_or_keys(k, seq_dim = seq_dim)\n",
    "\n",
    "        rotated_q = rotated_q.type(q.dtype)\n",
    "        rotated_k = rotated_k.type(k.dtype)\n",
    "\n",
    "        return rotated_q, rotated_k\n",
    "\n",
    "    def rotate_queries_and_keys(self, q, k, seq_dim = None):\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        assert self.use_xpos\n",
    "        device, dtype, seq_len = q.device, q.dtype, q.shape[seq_dim]\n",
    "\n",
    "        seq = self.get_seq_pos(seq_len, dtype = dtype, device = device)\n",
    "        freqs = self.forward(lambda: seq, cache_key = f'freqs:{seq_len}')\n",
    "        scale = self.get_scale(lambda: seq, cache_key = f'scale:{seq_len}').to(dtype)\n",
    "\n",
    "        if seq_dim == -3:\n",
    "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
    "            scale = rearrange(scale, 'n d -> n 1 d')\n",
    "\n",
    "        rotated_q = apply_rotary_emb(freqs, q, scale = scale, seq_dim = seq_dim)\n",
    "        rotated_k = apply_rotary_emb(freqs, k, scale = scale ** -1, seq_dim = seq_dim)\n",
    "\n",
    "        rotated_q = rotated_q.type(q.dtype)\n",
    "        rotated_k = rotated_k.type(k.dtype)\n",
    "\n",
    "        return rotated_q, rotated_k\n",
    "\n",
    "    def get_scale(self, t, cache_key = None):\n",
    "        assert self.use_xpos\n",
    "\n",
    "        if exists(cache_key) and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        if callable(t):\n",
    "            t = t()\n",
    "\n",
    "        scale = 1.\n",
    "        if self.use_xpos:\n",
    "            power = (t - len(t) // 2) / self.scale_base\n",
    "            scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "            scale = torch.cat((scale, scale), dim = -1)\n",
    "\n",
    "        if exists(cache_key):\n",
    "            self.cache[cache_key] = scale\n",
    "\n",
    "        return scale\n",
    "\n",
    "    @autocast(enabled = False)\n",
    "    def forward(self, t, cache_key = None):\n",
    "        should_cache = not self.learned_freq and exists(cache_key)\n",
    "\n",
    "        if should_cache and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        if callable(t):\n",
    "            t = t()\n",
    "\n",
    "        freqs = self.freqs\n",
    "\n",
    "        freqs = einsum('..., f -> ... f', t.type(freqs.dtype), freqs)\n",
    "        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n",
    "\n",
    "        if should_cache:\n",
    "            self.cache[cache_key] = freqs\n",
    "\n",
    "        return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d90bb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "697228ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_emb = RotaryEmbedding(dim = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32756fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affc556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(MHSelfAttention, self).__init__()\n",
    "        \n",
    "        d_model = config.n_embd\n",
    "        n_head = config.n_head\n",
    "        \n",
    "        bias = config.bias\n",
    "        dropout= config.dropout\n",
    "        \n",
    "        \n",
    "        assert d_model % n_head == 0\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.b_bias = bias\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.c_attn = nn.Linear(3 * d_model,d_model)\n",
    "        self.c_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #adding flash attention referenced from NANOGPT\n",
    "        \n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        q = self.query(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        q = rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = rotary_emb.rotate_queries_or_keys(k)\n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            attn_ = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "            attn_ = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            attn_weights = F.softmax(attn_, dim=-1)\n",
    "            attn_weights = self.att_dropout(attn_weights) \n",
    "            out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.projection(out)\n",
    "        out = self.att_dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459fed58",
   "metadata": {},
   "source": [
    "PositionwiseFeedForward layer (  as pointwise in assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732208a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PointwiseFeedForward, self).__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)  #fc layer\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)  #projection layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e808e16",
   "metadata": {},
   "source": [
    "Normalization layer is defined . We can also use Functional's layernorm for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96b6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model,bias, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model)) if bias else none\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e6f0c",
   "metadata": {},
   "source": [
    "Defining a data class decorator so that it can handle input values as well as pre trained values properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2e2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79468a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2Layer, self).__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MHSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = PointwiseFeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cfd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(GPT2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "        self.drop = nn.Dropout(config.dropout),\n",
    "        self.h = nn.ModuleList([GPT2Layer(config) for _ in range(config.n_layer)]),\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        positions = torch.arange(0, t, dtype=torch.long, device=device) \n",
    "\n",
    "        # forward the GPT model itself\n",
    "        \n",
    "        tok_emb = self.wte(x) \n",
    "        \n",
    "        pos_emb = self.wpe(pos) \n",
    "        \n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for layer in self.h:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    \n",
    "    def load_pretrained_weights(self, model_name='gpt2'):\n",
    "        # Load pretrained weights from Hugging Face model\n",
    "        \n",
    "        config_ = dict(bias= True, n_layer=12, n_head=12, n_embd=768, vocab_size=50257 , block_size= 1024 )\n",
    "        config = GPTConfig(**config_)\n",
    "        pretrained_model = GPT2Model.from_pretrained(model_name,resume_download=True)\n",
    "        \n",
    "        state_dict = GPT2(config).state_dict()\n",
    "        sd_keys = state_dict.keys()\n",
    "        \n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "        \n",
    "\n",
    "        state_dict_pretrained = pretrained_model.state_dict()\n",
    "        sdt_keys = state_dict_pretrained.keys()\n",
    "        \n",
    "        \n",
    "        \n",
    "        sdt_keys = [k for k in sdt_keys if not k.endswith('.attn.masked_bias')] \n",
    "        sdt_keys = [k for k in sdt_keys if not k.endswith('.attn.bias')]\n",
    "        \n",
    "        \n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        print(sd_keys)\n",
    "        print(sdt_keys)\n",
    "        \n",
    "        for k in sdt_keys:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                # assert state_dict_pretrained[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    state_dict[k].copy_(state_dict_pretrained[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "               # assert state_dict_pretrained[k].shape == state_dict[k].shape\n",
    "                with torch.no_grad():\n",
    "                    state_dict[k].copy_(state_dict_pretrained[k])\n",
    "\n",
    "        return model\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7073362f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesha\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m my_gpt_model \u001b[38;5;241m=\u001b[39m GPT2(config)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load pretrained weights from Hugging Face GPT-2\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmy_gpt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Perform a sample prediction\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 58\u001b[0m, in \u001b[0;36mGPT2.load_pretrained_weights\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     56\u001b[0m config_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(bias\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, n_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, n_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, n_embd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50257\u001b[39m , block_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m )\n\u001b[0;32m     57\u001b[0m config \u001b[38;5;241m=\u001b[39m GPTConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_)\n\u001b[1;32m---> 58\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m GPT2(config)\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m     61\u001b[0m sd_keys \u001b[38;5;241m=\u001b[39m state_dict\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2230\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2230\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit:\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:679\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:679\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mGPT2Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:367\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[1;34m(self, config, layer_idx)\u001b[0m\n\u001b[0;32m    364\u001b[0m inner_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mn_inner \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mn_inner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m hidden_size\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39madd_cross_attention:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:158\u001b[0m, in \u001b[0;36mGPT2Attention.__init__\u001b[1;34m(self, config, is_cross_attention, layer_idx)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_attn \u001b[38;5;241m=\u001b[39m Conv1D(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_attn \u001b[38;5;241m=\u001b[39m \u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj \u001b[38;5;241m=\u001b[39m Conv1D(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39mattn_pdrop)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pytorch_utils.py:106\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[1;34m(self, nf, nx)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf \u001b[38;5;241m=\u001b[39m nf\n\u001b[0;32m    105\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(nx, nf)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(w)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(nf))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "config= GPTConfig()\n",
    "my_gpt_model = GPT2(config)\n",
    "\n",
    "# Load pretrained weights from Hugging Face GPT-2\n",
    "my_gpt_model.load_pretrained_weights()\n",
    "\n",
    "# Perform a sample prediction\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "input_sequence = tokenizer.encode(\"Hello, how are you today?\", return_tensors=\"pt\")\n",
    "output_hidden_states = my_gpt_model(input_sequence)\n",
    "\n",
    "print(\"Input Sequence:\", input_sequence)\n",
    "print(\"Output Hidden States Shape:\", output_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b04077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9a71753",
   "metadata": {},
   "source": [
    "keys mismatch between gpt2-small and my implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3805e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
