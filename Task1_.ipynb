{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff381f9",
   "metadata": {},
   "source": [
    "**Task 1 | GPT-2 Model & Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbc5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#for testing of my implementation\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "import torch.utils.checkpoint\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e840b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fa3e4",
   "metadata": {},
   "source": [
    "Errors faced in TASK 1:\n",
    "\n",
    "1.Keys of pre trained model weights were not matching my implementation both in numbers and name so I have to rename and shift them according to huggingface gpt-2 125 parameters.\n",
    "Facing error in pretraining because of name difference between hugging face gpt2 . I used nano bot as reference but some names are different thats why the issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b8ea8b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RotaryEmbedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rotary_emb \u001b[38;5;241m=\u001b[39m \u001b[43mRotaryEmbedding\u001b[49m(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RotaryEmbedding' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I tried to Define tokenized + positonal embedding seperately for our sequence, Here I took default vocab_size  as 50304\n",
    "because it is used in original gpt2 models at best efficiency & block size as 1024 as maximum sequence length.(num_embeddings). \n",
    "We add dropout layer to avoid overfitting, by performing regularization.\n",
    "( I avoided assertingparameters and inputs as this is a simple model for demonstrating my understanding of transformors architecture)..  \n",
    "\n",
    "\n",
    "\n",
    "class TP_Embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super(TP_Embedding, self).__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.n_embd)\n",
    "        self.positional_embedding = nn.Embedding(num_embeddings=config.block_size, embedding_dim=config.n_embd) \n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        positions = torch.arange(0, x.size(1)).unsqueeze(0).repeat(x.size(0), 1).to(x.device)\n",
    "        \n",
    "        token_embedded = self.token_embedding(x)\n",
    "        positional_embedded = self.positional_embedding(positions)\n",
    "        \n",
    "        x = self.dropout(positional_embedded)\n",
    "        \n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "#had to comment because no. of keys were not matching my implementation for some reason\n",
    "#so shifted this class methods directly into the GPT 2 model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523188e",
   "metadata": {},
   "source": [
    "implementation of a multi head self attention layer . I am avoiding pruning for simplicity and time constraint purposes and will try to use a vanilla layer same as nanogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affc556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(MHSelfAttention, self).__init__()\n",
    "        \n",
    "        d_model = config.n_embd\n",
    "        n_head = config.n_head\n",
    "        \n",
    "        bias = config.bias\n",
    "        dropout= config.dropout\n",
    "        \n",
    "        \n",
    "        assert d_model % n_head == 0\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.b_bias = bias\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.c_attn = nn.Linear(3 * d_model,d_model)\n",
    "        self.c_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #adding flash attention referenced from NANOGPT\n",
    "        \n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        q = self.query(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            attn_ = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "            attn_ = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            attn_weights = F.softmax(attn_, dim=-1)\n",
    "            attn_weights = self.att_dropout(attn_weights) \n",
    "            out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.projection(out)\n",
    "        out = self.att_dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459fed58",
   "metadata": {},
   "source": [
    "PositionwiseFeedForward layer (  as pointwise in assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "732208a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PointwiseFeedForward, self).__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)  #fc layer\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)  #projection layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e808e16",
   "metadata": {},
   "source": [
    "Normalization layer is defined . We can also use Functional's layernorm for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a96b6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model,bias, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model)) if bias else none\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e6f0c",
   "metadata": {},
   "source": [
    "Defining a data class decorator so that it can handle input values as well as pre trained values properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d2e2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d79468a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2Layer, self).__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MHSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = PointwiseFeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82cfd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(GPT2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "        self.drop = nn.Dropout(config.dropout),\n",
    "        self.h = nn.ModuleList([GPT2Layer(config) for _ in range(config.n_layer)]),\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        positions = torch.arange(0, t, dtype=torch.long, device=device) \n",
    "\n",
    "        # forward the GPT model itself\n",
    "        \n",
    "        tok_emb = self.wte(x) \n",
    "        \n",
    "        pos_emb = self.wpe(pos) \n",
    "        \n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for layer in self.h:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    \n",
    "    def load_pretrained_weights(self, model_name='gpt2'):\n",
    "        # Load pretrained weights from Hugging Face model\n",
    "        \n",
    "        config_ = dict(bias= True, n_layer=12, n_head=12, n_embd=768, vocab_size=50257 , block_size= 1024 )\n",
    "        config = GPTConfig(**config_)\n",
    "        pretrained_model = GPT2Model.from_pretrained(model_name,resume_download=True)\n",
    "        \n",
    "        state_dict = GPT2(config).state_dict()\n",
    "        sd_keys = state_dict.keys()\n",
    "        \n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "        \n",
    "\n",
    "        state_dict_pretrained = pretrained_model.state_dict()\n",
    "        sdt_keys = state_dict_pretrained.keys()\n",
    "        \n",
    "        \n",
    "        \n",
    "        sdt_keys = [k for k in sdt_keys if not k.endswith('.attn.masked_bias')] \n",
    "        sdt_keys = [k for k in sdt_keys if not k.endswith('.attn.bias')]\n",
    "        \n",
    "        \n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        print(sd_keys)\n",
    "        print(sdt_keys)\n",
    "        \n",
    "        for k in sdt_keys:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                # assert state_dict_pretrained[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    state_dict[k].copy_(state_dict_pretrained[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "               # assert state_dict_pretrained[k].shape == state_dict[k].shape\n",
    "                with torch.no_grad():\n",
    "                    state_dict[k].copy_(state_dict_pretrained[k])\n",
    "\n",
    "        return model\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7073362f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesha\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "['lm_head.weight']\n",
      "['wte.weight', 'wpe.weight', 'h.0.ln_1.weight', 'h.0.ln_1.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_attn.bias', 'h.0.attn.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.0.ln_2.weight', 'h.0.ln_2.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.1.ln_1.weight', 'h.1.ln_1.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_proj.weight', 'h.1.attn.c_proj.bias', 'h.1.ln_2.weight', 'h.1.ln_2.bias', 'h.1.mlp.c_fc.weight', 'h.1.mlp.c_fc.bias', 'h.1.mlp.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.2.ln_1.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.2.ln_2.weight', 'h.2.ln_2.bias', 'h.2.mlp.c_fc.weight', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.3.ln_1.weight', 'h.3.ln_1.bias', 'h.3.attn.c_attn.weight', 'h.3.attn.c_attn.bias', 'h.3.attn.c_proj.weight', 'h.3.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.3.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.3.mlp.c_proj.bias', 'h.4.ln_1.weight', 'h.4.ln_1.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.4.ln_2.weight', 'h.4.ln_2.bias', 'h.4.mlp.c_fc.weight', 'h.4.mlp.c_fc.bias', 'h.4.mlp.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.5.ln_1.weight', 'h.5.ln_1.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.5.attn.c_proj.bias', 'h.5.ln_2.weight', 'h.5.ln_2.bias', 'h.5.mlp.c_fc.weight', 'h.5.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.5.mlp.c_proj.bias', 'h.6.ln_1.weight', 'h.6.ln_1.bias', 'h.6.attn.c_attn.weight', 'h.6.attn.c_attn.bias', 'h.6.attn.c_proj.weight', 'h.6.attn.c_proj.bias', 'h.6.ln_2.weight', 'h.6.ln_2.bias', 'h.6.mlp.c_fc.weight', 'h.6.mlp.c_fc.bias', 'h.6.mlp.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.7.ln_1.bias', 'h.7.attn.c_attn.weight', 'h.7.attn.c_attn.bias', 'h.7.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.7.ln_2.weight', 'h.7.ln_2.bias', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.7.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.8.ln_1.weight', 'h.8.ln_1.bias', 'h.8.attn.c_attn.weight', 'h.8.attn.c_attn.bias', 'h.8.attn.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.8.ln_2.weight', 'h.8.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.8.mlp.c_fc.bias', 'h.8.mlp.c_proj.weight', 'h.8.mlp.c_proj.bias', 'h.9.ln_1.weight', 'h.9.ln_1.bias', 'h.9.attn.c_attn.weight', 'h.9.attn.c_attn.bias', 'h.9.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.9.ln_2.weight', 'h.9.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.9.mlp.c_fc.bias', 'h.9.mlp.c_proj.weight', 'h.9.mlp.c_proj.bias', 'h.10.ln_1.weight', 'h.10.ln_1.bias', 'h.10.attn.c_attn.weight', 'h.10.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.10.ln_2.bias', 'h.10.mlp.c_fc.weight', 'h.10.mlp.c_fc.bias', 'h.10.mlp.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.11.ln_1.bias', 'h.11.attn.c_attn.weight', 'h.11.attn.c_attn.bias', 'h.11.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.11.ln_2.weight', 'h.11.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.11.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.11.mlp.c_proj.bias', 'ln_f.weight', 'ln_f.bias']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wte.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load pretrained weights from Hugging Face GPT-2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"before this model is working but the pretrained weights are having key differences which need to be solved\"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmy_gpt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Perform a sample prediction\u001b[39;00m\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 89\u001b[0m, in \u001b[0;36mGPT2.load_pretrained_weights\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;66;03m# vanilla copy over the other parameters\u001b[39;00m\n\u001b[0;32m     87\u001b[0m        \u001b[38;5;66;03m# assert state_dict_pretrained[k].shape == state_dict[k].shape\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 89\u001b[0m             \u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(state_dict_pretrained[k])\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mKeyError\u001b[0m: 'wte.weight'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "config= GPTConfig()\n",
    "my_gpt_model = GPT2(config)\n",
    "\n",
    "# Load pretrained weights from Hugging Face GPT-2\n",
    "\n",
    "\"\"\"before this model is working but the pretrained weights are having key differences which need to be solved\"\"\"\n",
    "my_gpt_model.load_pretrained_weights()\n",
    "\n",
    "# Perform a sample prediction\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "input_sequence = tokenizer.encode(\"Hello, how are you today?\", return_tensors=\"pt\")\n",
    "output_hidden_states = my_gpt_model(input_sequence)\n",
    "\n",
    "print(\"Input Sequence:\", input_sequence)\n",
    "print(\"Output Hidden States Shape:\", output_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b04077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9a71753",
   "metadata": {},
   "source": [
    "keys mismatch between gpt2-small and my implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3805e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
