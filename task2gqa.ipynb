{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff381f9",
   "metadata": {},
   "source": [
    "**Task 2 **| \n",
    "2.GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbc5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#for testing of my implementation\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum, rearrange\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "\n",
    "import torch.utils.checkpoint\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e840b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aea9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_gqa(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    dropout: float = 0.0,\n",
    "    scale: Optional[float] = None,\n",
    "    mask: Optional[Tensor] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    need_weights: bool = False,\n",
    "    average_attn_weights: bool = False,\n",
    "    force_grouped: bool = False,\n",
    "):\n",
    "    if (mask is not None) and (is_causal is not None):\n",
    "        raise ValueError(\n",
    "            \"Only one of 'mask' and 'is_causal' should be provided, but got both.\"\n",
    "        )\n",
    "    elif not query.ndim == key.ndim == value.ndim == 4:\n",
    "        raise ValueError(\n",
    "            f\"Expected query, key, and value to be 4-dimensional, but got shapes \"\n",
    "            f\"{query.shape}, {key.shape}, and {value.shape}.\"\n",
    "        )\n",
    "\n",
    "    # Move sequence length dimension to axis 2.\n",
    "    # This makes the attention operations below *much* faster.\n",
    "    query = rearrange(query, \"b n h d -> b h n d\")\n",
    "    key = rearrange(key, \"b s h d -> b h s d\")\n",
    "    value = rearrange(value, \"b s h d -> b h s d\")\n",
    "\n",
    "    bq, hq, nq, dq = query.shape\n",
    "    bk, hk, nk, dk = key.shape\n",
    "    bv, hv, nv, dv = value.shape\n",
    "    if not (bq == bk == bv and dq == dk == dv):\n",
    "        raise ValueError(\n",
    "            \"Expected query, key, and value to have the same batch size (dim=0) and \"\n",
    "            f\"embedding dimension (dim=3), but got query: {query.shape}, \"\n",
    "            f\"key: {key.shape}, and value: {value.shape}.\"\n",
    "        )\n",
    "    elif (hk != hv) or (nk != nv):\n",
    "        raise ValueError(\n",
    "            \"Expected key and value to have the same size in dimensions 1 and 2, but \"\n",
    "            f\"got key: {key.shape} and value: {value.shape}.\"\n",
    "        )\n",
    "    elif hq % hk != 0:\n",
    "        raise ValueError(\n",
    "            \"Expected query heads to be a multiple of key/value heads, but got \"\n",
    "            f\"query: {query.shape} and key/value: {key.shape}.\"\n",
    "        )\n",
    "\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** 0.5\n",
    "    query = query / scale\n",
    "\n",
    "    num_head_groups = hq // hk\n",
    "    if num_head_groups > 1 or force_grouped:\n",
    "        # Separate the query heads into 'num_head_groups' chunks, and fold the group\n",
    "        # dimension into the batch dimension.  This allows us to compute the attention\n",
    "        # for each head in parallel, then sum over all of the groups at the end.\n",
    "        query = rearrange(query, \"b (h g) n d -> b g h n d\", g=num_head_groups)\n",
    "        similarity = einsum(query, key, \"b g h n d, b h s d -> b h n s\")\n",
    "    else:\n",
    "        # If the number of query/key heads is equal, we can skip grouping the queries,\n",
    "        # and just use the standard sdot product attention.\n",
    "        similarity = einsum(query, key, \"b h n d, b h s d -> b h n s\")\n",
    "\n",
    "    if is_causal:\n",
    "        # Mask out the upper triangular portion of the attention matrix. This prevents\n",
    "        # the model from attending to tokens in the future.\n",
    "        mask = torch.ones(\n",
    "            (bq, nq, nk),\n",
    "            device=query.device,\n",
    "            dtype=torch.bool,\n",
    "        ).tril_()\n",
    "\n",
    "    if mask is not None:\n",
    "        # Expand mask to match the shape of the attention matrix.\n",
    "        # If mask is 2D, assume that it is applied to the key/value sequence dimension.\n",
    "        # Else if mask is 3D, assume that it is applied to the query/key/value sequence\n",
    "        # dimension for all attention heads.\n",
    "        #\n",
    "        # Users could also provide a 4D mask, which is applied to the query/key/value\n",
    "        # sequence dimension for each attention head (though I don't have a particular\n",
    "        # use case in mind for that).\n",
    "        if mask.ndim == 2:\n",
    "            mask = rearrange(mask, \"b s -> b () () s\")\n",
    "        elif mask.ndim == 3:\n",
    "            mask = rearrange(mask, \"b n s -> b () n s\")\n",
    "        # Mask similarity values by setting them to negative infinity.  This guarantees\n",
    "        # that they will not contribute to the softmax computation below.\n",
    "        similarity.masked_fill_(~mask, torch.finfo(similarity.dtype).min)\n",
    "\n",
    "    attention = F.softmax(similarity / scale, dim=-1)\n",
    "    if dropout > 0.0:\n",
    "        attention = F.dropout(attention, p=dropout)\n",
    "\n",
    "    # Apply attention matrix to the value Tensor.\n",
    "    out = einsum(attention, value, \"b h n s, b h s d -> b h n d\")\n",
    "    # Move head dimension back to axis 2\n",
    "    out = rearrange(out, \"b h n d -> b n h d\")\n",
    "\n",
    "    attn_weights: Optional[Tensor] = None\n",
    "    if need_weights:\n",
    "        # Move the sequence dimensions back to positions 1, 2.  Move the head dimension\n",
    "        # to position 3.  This more closely matches the return shape of the attention\n",
    "        # output: (b, n, h, d).\n",
    "        attn_weights = rearrange(attention, \"b h n s -> b n s h\")\n",
    "        if average_attn_weights:\n",
    "            attn_weights = attn_weights.mean(dim=1)\n",
    "\n",
    "    return out, attn_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8ea8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadGQA(nn.Module):\n",
    "    \"\"\"Multi-head grouped query attention (GQA) layer.\n",
    "\n",
    "    Reference:\n",
    "        \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n",
    "        https://arxiv.org/pdf/2305.13245v1.pdf\n",
    "\n",
    "    GQA is a variant of multihead attention (MHA) that uses fewer write heads\n",
    "    (key / value) than query heads.  GQA can be viewed as a generalization of\n",
    "    multi-query attention (MQA), which uses a single write head. GQA and MQA give\n",
    "    significant speedups over standard MHA in decoder layers, with minimal loss in\n",
    "    accuracy. In the paper, GQA is shown to be more accurate than MQA, while still\n",
    "    having a significant speedup over MHA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        query_heads: int,\n",
    "        kv_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        layer_norm: bool = True,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        gamma_init: float = 1.0,\n",
    "        device: Optional[Union[torch.device, str]] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.query_heads = query_heads\n",
    "        self.kv_heads = kv_heads\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm = layer_norm\n",
    "        self.gamma_init = gamma_init\n",
    "\n",
    "        if self.query_heads % self.kv_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"query_heads ({query_heads}) must be divisible by \"\n",
    "                f\"kv_heads ({kv_heads})\"\n",
    "            )\n",
    "        elif (embed_dim % self.query_heads != 0) or (embed_dim % self.kv_heads != 0):\n",
    "            raise ValueError(\n",
    "                f\"embed_dim ({embed_dim}) must be divisible by \"\n",
    "                f\"query_heads ({query_heads}) and kv_heads ({kv_heads})\"\n",
    "            )\n",
    "\n",
    "        head_dim = embed_dim // query_heads\n",
    "        if not head_dim % 8 == 0:\n",
    "            raise ValueError(\n",
    "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be divisible by 8\"\n",
    "            )\n",
    "        if not head_dim <= 128:\n",
    "            raise ValueError(\n",
    "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be <= 128\"\n",
    "            )\n",
    "\n",
    "        # Query projection layer is the same as in vanilla MHA.\n",
    "        self.q_proj = nn.Linear(\n",
    "            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        # Key/value projection layers have a smaller output dimension, so that\n",
    "        # the we have fewer key/value attention heads after reshaping.\n",
    "        kv_embed_dim = embed_dim // query_heads * kv_heads\n",
    "        self.k_proj = nn.Linear(\n",
    "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        self.norm: Optional[nn.LayerNorm] = None\n",
    "        if layer_norm:\n",
    "            self.norm = nn.LayerNorm(\n",
    "                kv_embed_dim, eps=layer_norm_eps, device=device, dtype=dtype\n",
    "            )\n",
    "        # Grouped attention output will have the same embedding dimension as the\n",
    "        # key/value Tensors.  So the output projection layer needs to accept the\n",
    "        # same dimension (kv_embed_dim).\n",
    "        self.out_proj = nn.Linear(\n",
    "            kv_embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.q_proj.weight)\n",
    "        if self.q_proj.bias is not None:\n",
    "            nn.init.constant_(self.q_proj.bias, 0)\n",
    "        nn.init.xavier_normal_(self.k_proj.weight)\n",
    "        if self.k_proj.bias is not None:\n",
    "            nn.init.constant_(self.k_proj.bias, 0)\n",
    "\n",
    "        # NOTE: We follow the initialization strategy from MAGNETO.  See:\n",
    "        # https://arxiv.org/pdf/2210.06423.pdf, Fig. 2\n",
    "        # Gain (self.gamma_init) should be provided as a keyword argument when\n",
    "        # initializing the larger Transformer model, since it requires knowledge\n",
    "        # of the number of encoder/decoder layers in the model.\n",
    "\n",
    "        nn.init.xavier_normal_(self.v_proj.weight, gain=self.gamma_init)\n",
    "        if self.v_proj.bias is not None:\n",
    "            nn.init.constant_(self.v_proj.bias, 0)\n",
    "        nn.init.xavier_normal_(self.out_proj.weight, gain=self.gamma_init)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Tensor,\n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        need_weights: bool = False,\n",
    "        # TODO\n",
    "        # attn_mask: Optional[Tensor] = None,\n",
    "        is_causal: bool = False,\n",
    "        average_attn_weights: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # Notation:\n",
    "        #   b - batch size\n",
    "        #   n - sequence length\n",
    "        #   h - number of heads\n",
    "        #   d - embedding dimension\n",
    "        #\n",
    "        # Input shape: (b, n, d)\n",
    "        q: Tensor = self.q_proj(query)\n",
    "        k: Tensor = self.k_proj(key)\n",
    "        v: Tensor = self.v_proj(value)\n",
    "\n",
    "        # Unfold 'd' dimension into 'h' separate attention heads.\n",
    "        q = rearrange(q, \"b n (h d) -> b n h d\", h=self.query_heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
    "        # Apply attention, then fold 'h' attention heads back into 'd'.\n",
    "        x, attn = scaled_dot_product_gqa(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v,\n",
    "            # TODO\n",
    "            # mask=attn_mask,\n",
    "            is_causal=is_causal,\n",
    "            need_weights=need_weights,\n",
    "            average_attn_weights=average_attn_weights,\n",
    "            force_grouped=False,\n",
    "        )\n",
    "        x = rearrange(x, \"b n h d -> b n (h d)\")\n",
    "\n",
    "        if self.layer_norm:\n",
    "            assert self.norm is not None\n",
    "            x = self.norm(x)\n",
    "        # Linear projection on attention outputs.\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523188e",
   "metadata": {},
   "source": [
    "implementation of a multi head self attention layer . I am avoiding pruning for simplicity and time constraint purposes and will try to use a vanilla layer same as nanogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc556d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "459fed58",
   "metadata": {},
   "source": [
    "PositionwiseFeedForward layer (  as pointwise in assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732208a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PointwiseFeedForward, self).__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)  #fc layer\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)  #projection layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e808e16",
   "metadata": {},
   "source": [
    "Normalization layer is defined . We can also use Functional's layernorm for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96b6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model,bias, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model)) if bias else none\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e6f0c",
   "metadata": {},
   "source": [
    "Defining a data class decorator so that it can handle input values as well as pre trained values properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2e2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d79468a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2Layer, self).__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MHSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = PointwiseFeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cfd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(GPT2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "        self.drop = nn.Dropout(config.dropout),\n",
    "        self.h = nn.ModuleList([GPT2Layer(config) for _ in range(config.n_layer)]),\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        positions = torch.arange(0, t, dtype=torch.long, device=device) \n",
    "\n",
    "        # forward the GPT model itself\n",
    "        \n",
    "        tok_emb = self.wte(x) \n",
    "        \n",
    "        pos_emb = self.wpe(pos) \n",
    "        \n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for layer in self.h:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    \n",
    "    def load_pretrained_weights(self, model_name='gpt2'):\n",
    "        # Load pretrained weights from Hugging Face model\n",
    "        \n",
    "        config_ = dict(bias= True, n_layer=12, n_head=12, n_embd=768, vocab_size=50257 , block_size= 1024 )\n",
    "        config = GPTConfig(**config_)\n",
    "        pretrained_model = GPT2Model.from_pretrained(model_name,resume_download=True)\n",
    "        \n",
    "        state_dict = GPT2(config).state_dict()\n",
    "        sd_keys = state_dict.keys()\n",
    "        \n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "        \n",
    "\n",
    "        state_dict_pretrained = pretrained_model.state_dict()\n",
    "        sdt_keys = state_dict_pretrained.keys()\n",
    "        \n",
    "        \n",
    "        \n",
    "        sdt_keys = [k for k in sdt_keys if not k.endswith('.attn.masked_bias')] \n",
    "        sdt_keys = [k for k in sdt_keys if not k.endswith('.attn.bias')]\n",
    "        \n",
    "        \n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        print(sd_keys)\n",
    "        print(sdt_keys)\n",
    "        \n",
    "        for k in sdt_keys:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                # assert state_dict_pretrained[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    state_dict[k].copy_(state_dict_pretrained[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "               # assert state_dict_pretrained[k].shape == state_dict[k].shape\n",
    "                with torch.no_grad():\n",
    "                    state_dict[k].copy_(state_dict_pretrained[k])\n",
    "\n",
    "        return model\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7073362f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesha\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "['lm_head.weight']\n",
      "['wte.weight', 'wpe.weight', 'h.0.ln_1.weight', 'h.0.ln_1.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_attn.bias', 'h.0.attn.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.0.ln_2.weight', 'h.0.ln_2.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.1.ln_1.weight', 'h.1.ln_1.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_proj.weight', 'h.1.attn.c_proj.bias', 'h.1.ln_2.weight', 'h.1.ln_2.bias', 'h.1.mlp.c_fc.weight', 'h.1.mlp.c_fc.bias', 'h.1.mlp.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.2.ln_1.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.2.ln_2.weight', 'h.2.ln_2.bias', 'h.2.mlp.c_fc.weight', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.3.ln_1.weight', 'h.3.ln_1.bias', 'h.3.attn.c_attn.weight', 'h.3.attn.c_attn.bias', 'h.3.attn.c_proj.weight', 'h.3.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.3.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.3.mlp.c_proj.bias', 'h.4.ln_1.weight', 'h.4.ln_1.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.4.ln_2.weight', 'h.4.ln_2.bias', 'h.4.mlp.c_fc.weight', 'h.4.mlp.c_fc.bias', 'h.4.mlp.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.5.ln_1.weight', 'h.5.ln_1.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.5.attn.c_proj.bias', 'h.5.ln_2.weight', 'h.5.ln_2.bias', 'h.5.mlp.c_fc.weight', 'h.5.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.5.mlp.c_proj.bias', 'h.6.ln_1.weight', 'h.6.ln_1.bias', 'h.6.attn.c_attn.weight', 'h.6.attn.c_attn.bias', 'h.6.attn.c_proj.weight', 'h.6.attn.c_proj.bias', 'h.6.ln_2.weight', 'h.6.ln_2.bias', 'h.6.mlp.c_fc.weight', 'h.6.mlp.c_fc.bias', 'h.6.mlp.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.7.ln_1.bias', 'h.7.attn.c_attn.weight', 'h.7.attn.c_attn.bias', 'h.7.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.7.ln_2.weight', 'h.7.ln_2.bias', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.7.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.8.ln_1.weight', 'h.8.ln_1.bias', 'h.8.attn.c_attn.weight', 'h.8.attn.c_attn.bias', 'h.8.attn.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.8.ln_2.weight', 'h.8.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.8.mlp.c_fc.bias', 'h.8.mlp.c_proj.weight', 'h.8.mlp.c_proj.bias', 'h.9.ln_1.weight', 'h.9.ln_1.bias', 'h.9.attn.c_attn.weight', 'h.9.attn.c_attn.bias', 'h.9.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.9.ln_2.weight', 'h.9.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.9.mlp.c_fc.bias', 'h.9.mlp.c_proj.weight', 'h.9.mlp.c_proj.bias', 'h.10.ln_1.weight', 'h.10.ln_1.bias', 'h.10.attn.c_attn.weight', 'h.10.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.10.ln_2.bias', 'h.10.mlp.c_fc.weight', 'h.10.mlp.c_fc.bias', 'h.10.mlp.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.11.ln_1.bias', 'h.11.attn.c_attn.weight', 'h.11.attn.c_attn.bias', 'h.11.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.11.ln_2.weight', 'h.11.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.11.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.11.mlp.c_proj.bias', 'ln_f.weight', 'ln_f.bias']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wte.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load pretrained weights from Hugging Face GPT-2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"before this model is working but the pretrained weights are having key differences which need to be solved\"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmy_gpt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Perform a sample prediction\u001b[39;00m\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 89\u001b[0m, in \u001b[0;36mGPT2.load_pretrained_weights\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;66;03m# vanilla copy over the other parameters\u001b[39;00m\n\u001b[0;32m     87\u001b[0m        \u001b[38;5;66;03m# assert state_dict_pretrained[k].shape == state_dict[k].shape\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 89\u001b[0m             \u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(state_dict_pretrained[k])\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mKeyError\u001b[0m: 'wte.weight'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "config= GPTConfig()\n",
    "my_gpt_model = GPT2(config)\n",
    "\n",
    "# Load pretrained weights from Hugging Face GPT-2\n",
    "\n",
    "\"\"\"before this model is working but the pretrained weights are having key differences which need to be solved\"\"\"\n",
    "my_gpt_model.load_pretrained_weights()\n",
    "\n",
    "# Perform a sample prediction\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "input_sequence = tokenizer.encode(\"Hello, how are you today?\", return_tensors=\"pt\")\n",
    "output_hidden_states = my_gpt_model(input_sequence)\n",
    "\n",
    "print(\"Input Sequence:\", input_sequence)\n",
    "print(\"Output Hidden States Shape:\", output_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b04077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9a71753",
   "metadata": {},
   "source": [
    "keys mismatch between gpt2-small and my implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3805e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
